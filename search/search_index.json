{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kubauth","text":"<p>A Kubernetes-native OpenID Connect (OIDC) Identity Provider.</p>"},{"location":"#overview","title":"Overview","text":"<p>KubAuth is a fully-featured OIDC identity provider designed for Kubernetes environments. </p> <p>It stores users, groups, clients, and sessions as native Kubernetes resources, providing a scalable and cloud-native authentication solution.</p> <p>Can be fully autonomous, hosting its own User database or acting as a relay for external identity provider such as LDAP.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>OIDC Compliance: Supports standard OIDC flows including Authorization Code, and Resource Owner Password Credentials (ROPC)</li> <li>PKCE Support: Complete Proof Key for Code Exchange (PKCE) implementation with configurable enforcement</li> <li>Kubernetes-Native Storage: All data stored as Kubernetes Custom Resources (CRDs). NO DATABASE</li> <li>SSO Capabilities: Cross-application Single Sign-On with persistent sessions</li> <li>User &amp; Group Management: Fine-grained user authentication and group-based authorization. <ul> <li>Claims can be defined at users or group level.</li> <li>User profile can be built from several identity sources</li> </ul> </li> <li>Security First: bcrypt password hashing, JWT signing with persistent keys, secure session management</li> <li>Production Ready: Health checks, webhooks, and Helm chart deployment</li> </ul>"},{"location":"#kubauth-components","title":"Kubauth components","text":"<p>Kubauth is made from several subprojects:</p> <ul> <li>kubauth: The main project, the OIDC server with all its connectors.</li> <li>kc: A companion CLI tool.</li> <li>Two subprojects aimed to authenticate <code>kubectl</code> users with OIDC.<ul> <li>kubauth-apiserver: A kubernetes configuration tools to automate the k8s <code>apiserver</code> configuration.</li> <li>kubauth-kubeconfig: A tool to automate the <code>kubectl</code> client configuration.</li> </ul> </li> </ul>"},{"location":"20-installation/","title":"Installation","text":""},{"location":"20-installation/#prerequisite","title":"Prerequisite","text":"<p>Before you begin, make sure you meet the following prerequisites:</p> <ul> <li> <p>Kubectl Configuration: You should have a local client Kubernetes configuration with full administrative   rights on the target cluster.</p> </li> <li> <p>Certificate Manager: Ensure that the Certificate Manager is deployed in your target Kubernetes cluster, and a <code>ClusterIssuer</code> is defined for certificate management.</p> </li> <li> <p>Ingress Controller: An NGINX ingress controller should be deployed in your target Kubernetes cluster. With of course an upfront load balancer.</p> </li> <li> <p>Helm: Helm must be installed locally on your system.</p> </li> </ul> <p>Tip</p> <p>If you don't have an appropriate Kubernetes cluster at your disposal, you can deploy a Kind cluster on your local workstation</p>"},{"location":"20-installation/#kubauth-deployment","title":"Kubauth Deployment","text":"<p>The most straightforward and recommended method for installing Kubauth is by using the provided OCI Helm chart.</p> <p>As there are several required configuration variable, we suggest to use a 'values file' instead of setting variable on the command line.</p> <p>So, in your working folder, create a file like the following:</p> values.yaml <pre><code>oidc:\n  issuer: https://kubauth.ingress.kubo6.mbp\n  postLogoutURL: https://kubauth.ingress.kubo6.mbp/index\n\n  ingress:\n    host: kubauth.ingress.kubo6.mbp\n\n  server:\n    certificateIssuer: cluster-odp\n</code></pre> <p>Replace the values with your specific configuration:</p> <ul> <li><code>kubauth.ingress.kubo6.mbp</code>: Replace by the hostname used for accessing the <code>kubauth</code> service from outside the cluster. <p>Make sure to define this hostname in your DNS.</p> </li> <li><code>cluster-odp</code>: Replace by The ClusterIssuer from your Certificate Manager for certificate management.</li> </ul> <p>Notes</p> <p>This <code>values.yaml</code> file is the bare minimum configuration set. In subsequent chapters, more variables may be added. </p> <p>Then, you can deploy Kubauth with the following command.</p> <pre><code>helm -n kubauth upgrade -i kubauth --values ./values.yaml oci://quay.io/kubauth/charts/kubauth --version 0.1.2-snapshot --create-namespace --wait\n</code></pre> <p>After few seconds, verify the Kubauth server pod is running:</p> <p><pre><code>kubectl -n kubauth get pods\n</code></pre> <pre><code>NAME                       READY   STATUS    RESTARTS   AGE\nkubauth-5d4fdc6bc8-7rlb6   3/3     Running   0          55s\n</code></pre></p> <p>And check the Kubauth issuer URL is reachable:</p> <pre><code>curl https://kubauth.ingress.kubo6.mbp/.well-known/openid-configuration\n</code></pre>"},{"location":"20-installation/#kc-cli-tool-installation","title":"<code>kc</code> CLI tool installation","text":"<p>Download the <code>kc</code> CLI from the GitHub releases page and rename it to <code>kc</code>. Then make it executable and move it to your path:</p> <pre><code>mv kc_*_* kc\nchmod +x kc\nsudo mv kc /usr/local/bin/\n</code></pre> <p>Verify the installation:</p> <pre><code>kc version\n</code></pre>"},{"location":"30-user-guide/110-configuration/","title":"Configuration","text":"<p>Basic Kubauth configuration is based on:</p> <ul> <li>Helm chart variable</li> <li>Users and Groups as Kubernetes Custom Resources</li> <li>Client Application as Kubernetes Custom Resources.</li> </ul> <p>We suggest you apply the sample configuration described below as this will be used in subsequent chapters.</p>"},{"location":"30-user-guide/110-configuration/#user-creation","title":"User creation","text":"<p>With Kubauth users may be defined as Kubernetes Custom Resources.</p> <p>Here is a sample manifest which will create two users:</p> users.yaml <pre><code>---\napiVersion: kubauth.kubotal.io/v1alpha1\nkind: User\nmetadata:\n  name: jim\n  namespace: kubauth-users\nspec:\n  passwordHash: \"$2a$12$yJEo9EoYn/ylGS4PCamfNe8PReYH9IPumsw7rMTDi3glZjuA7dXMm\"  # jim123\n\n---\napiVersion: kubauth.kubotal.io/v1alpha1\nkind: User\nmetadata:\n  name: john\n  namespace: kubauth-users\nspec:\n  passwordHash: \"$2a$12$YjalsuGc6uuWtQqVuU/O.eW9L6QGU/vHk2wpvle4dsS7hC2Ic1F.q\"  # john123\n  name: John DOE\n  claims:\n    office: 208G\n  emails:\n    - johnd@mycompany.com\n</code></pre> <ul> <li>The resource name is the user login.</li> <li>User must be defined in a specific namespace (<code>kubauth-users</code>). This to allow control using k8s rbac.</li> <li>The only mandatory user attribute is its password, provided as a hash.</li> <li>A <code>name</code> attribute can be set with the user full name.</li> <li>A list of emails can be associated to each user.</li> <li>To each user can be associated a list of supplementary OIDC claims, which will be merged with the system provided one. More on this below</li> </ul> <p>Deploy the manifest on your clusters</p> <pre><code>kubectl apply -f users.yaml \n</code></pre> <p>You can now list the newly created users</p> <p><pre><code>kubectl -n kubauth-users get users.kubauth.kubotal.io\n</code></pre> <pre><code>NAME   USERNAME   EMAILS                    UID   COMMENT   DISABLED   AGE\njim                                                                    82m\njohn   John DOE   [\"johnd@mycompany.com\"]                              67m\n</code></pre></p> <p>Note</p> <p>We better provide the fully qualified name of the resource, as <code>user</code> may refer to several other Kind of CRD. For this reason, an alias (<code>kuser</code>) has also be defined for kubauth's users: <pre><code>kubectl -n kubauth-users get kusers\n</code></pre></p>"},{"location":"30-user-guide/110-configuration/#password-hash","title":"Password hash","text":"<p>The <code>kc</code> CLI tool provide a subcommand to generate the hash of a password:</p> <pre><code>kc hash jim123\n</code></pre> <pre><code>Secret: jim123\nHash: $2a$12$nSplFbbsGoI7LXdhJrKx0erRmIv.zkTftG82sQZA0.v3l1eCf.ey.\n\nUse this hash in your User 'passwordHash' field\n\nExample:\n  apiVersion: kubauth.kubotal.io/v1alpha1\n  kind: User\n  .....\n  spec:\n    passwordHash: \"$2a$12$nSplFbbsGoI7LXdhJrKx0erRmIv.zkTftG82sQZA0.v3l1eCf.ey.\"\n\nOr in your OidcClient 'hashedSecret' field\n\nExample:\n  apiVersion: kubauth.kubotal.io/v1alpha1\n  kind: OidcClient\n  .....\n  spec:\n    hashedSecret: \"$2a$12$nSplFbbsGoI7LXdhJrKx0erRmIv.zkTftG82sQZA0.v3l1eCf.ey.\"\n</code></pre> <p>Just cut/paste appropriate line in your user manifest.</p>"},{"location":"30-user-guide/110-configuration/#namespace","title":"Namespace","text":"<p>If, for any reason, you need to change the namespace storing users resources definition, this can be modified by setting a helm chart configuration value: </p> values.yaml <pre><code>oidc:\n  issuer: https://kubauth.ingress.kubo6.mbp\n  postLogoutURL: https://kubauth.ingress.kubo6.mbp/index\n  ....\n\nucrd:\n  namespace: kubauth-users\n  createNamespace: true\n</code></pre>"},{"location":"30-user-guide/110-configuration/#oidc-client-creation","title":"OIDC Client creation","text":"<p>In OIDC terminology, a 'client' is an application delegating user's authentication to an OIDC server. As such, it must be referenced in the server.</p> <p>With Kubauth, a client application is defined as a Kubernetes Custom resource.</p> <p>Here is a first sample:</p> client-public.yaml <pre><code>apiVersion: kubauth.kubotal.io/v1alpha1\nkind: OidcClient\nmetadata:\n  name: public\n  namespace: kubauth-oidc\nspec:\n  redirectURIs:\n    - \"http://127.0.0.1:9921/callback\"\n  grantTypes: [ \"refresh_token\", \"authorization_code\" ]\n  responseTypes: [ \"id_token\", \"code\", \"token\", \"id_token token\", \"code id_token\", \"code token\", \"code id_token token\" ]\n  scopes: [ \"openid\", \"offline\", \"profile\", \"groups\", \"email\", \"offline_access\" ]\n  description: A test OIDC public client\n  public: true\n\n  # hashedSecret: \"$2a$12$9vdc.xb3Zf4ts/C2pSvIOuGmFiv0EStBJWslaaycavblaIjYZ9Mia\"\n  # accessTokenLifespan: 1h\n  # refreshTokenLifespan: 1h\n  # idTokenLifespan: 1h\n</code></pre> <ul> <li>The resource name is the client ID.</li> <li>The <code>redirectURIs</code> list must be adjusted for each application. The value here is specific to the test client included in the <code>kc</code> cli. See below.</li> <li>The <code>grantTypes</code> list define which authorization flow will be accepted by this client definition.</li> <li>The <code>responseTypes</code> list define what kind of tokens or credentials the client can expects to receive from the authorization endpoint after the user authenticates.</li> <li>The <code>scopes</code> list define which scope can be requested by the application. </li> <li>This client is defined as <code>public</code>. As such no client secret need to be provided.For non-public client, a <code>secret</code> in hashed form must be provided, as in the commented line.    Use the <code>kc hash</code> command described previously to generate it.</li> </ul> <p>Apply this manifest:</p> <pre><code>kubectl apply -f client-public.yaml\n</code></pre> <p>You can list existing OIDC client:</p> <pre><code>kubectl -n kubauth-oidc get oidcclients\n</code></pre> <pre><code>NAME     PUB.   DISPLAY   DESCRIPTION                 LINK   AGE\npublic   true             A test OIDC public client          25m\n</code></pre> <p>This client will be used in next chapter, to test Tokens and Claims</p>"},{"location":"30-user-guide/110-configuration/#namespace_1","title":"Namespace","text":"<p>If, for any reason, you need to change the namespace storing clients resources definition, this can be modified by setting a helm chart configuration value:</p> values.yaml <pre><code>oidc:\n  issuer: https://kubauth.ingress.kubo6.mbp\n  postLogoutURL: https://kubauth.ingress.kubo6.mbp/index\n  ....\n  clients:\n    createNamespace: true\n    namespace: kubauth-oidc\n</code></pre>"},{"location":"30-user-guide/120-tokens-and-claims/","title":"Tokens and Claims","text":""},{"location":"30-user-guide/120-tokens-and-claims/#get-a-token","title":"Get a token","text":"<p>The Kubauth companion CLI application <code>kc</code> provide an embedded OIDC client application. Beside being used as here for testing installation, its aim is to provide a tool to fetch Access token or OIDC token, ready to be injected in whatever application.</p> <p>Launch the following command, after adjusting the issuerURL;</p> <pre><code>kc token --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public\n</code></pre> <p>Notes</p> <p>If you got an error like <code>tls: failed to verify certificate: x509:...</code>, this means the CA associated to the ClusterIssuer is not reckonized on your local work station.</p> <ul> <li>Add the <code>--insecureSkipVerify</code> open on the <code>kc token</code> command. You will also have to force your browser to accept such situation.</li> <li>Add the CA as a trusted one on your local workstation. You can extract it with a command like:   <pre><code>kubectl -n cert-manager get secret cluster-odp-ca \\\n  -o jsonpath='{.data.ca\\.crt}' | base64 -d &gt;./ca.crt \n</code></pre></li> </ul> <p>Your browser should open on the kubauth login page:</p> <p></p> <p>Log in using <code>jim/jim123</code>. You should land on a page like the following</p> <p></p> <p>Where you can copy the provided tokens from.</p> <p>Theses tokens are also in the CLI response:</p> <pre><code>Access token: ory_at_xLUfAhEGpFVWpMLdNEDZAj94hHFrHWjgOYB5g0Leh_k.0rgIzRGFOiIeGsMKnIZ74QL4Ve5vVOuEZyhA0402u8Y\nRefresh token: ory_rt_nU9NBZs4NtKTxVYVko1aqlJkAMF5MLBYjfiZbhVt9aE.THwsnTlqzIsWo5O1NAf1EbDhz7HdaqVHHwSTkWxrkqY\nID token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImY0Y2NkNDU0LWYzYTgtNDQ3Zi1hN2MzLTY3ZmY5MzUxMzZiMSIsInR5cCI6IkpXVCJ9.eyJhdF9oYXNoIjoiaGNBY2dtdmdBekJlSGgyODlkWHF3USIsImF1ZCI6WyJwdWJsaWMiXSwiYXV0aF90aW1lIjoxNzYxMzI2MDg2LCJhenAiOiJwdWJsaWMiLCJleHAiOjE3NjEzMjk2ODYsImlhdCI6MTc2MTMyNjA4NiwiaXNzIjoiaHR0cHM6Ly9rdWJhdXRoLmluZ3Jlc3Mua3VibzYubWJwIiwianRpIjoiZDZhYjkwODMtYTEzMi00YTNiLTlmMWItMzM2NWFhOTQ5MjQ2IiwicmF0IjoxNzYxMzI2MDg2LCJzdWIiOiJqaW0ifQ.Q8ZkF33jsUJDqLH98uqRgrFa2nwioRP1TO9n6QjX9XFr-1WmsKk9nEeHGAiASb1brQ3cSAmK8ta7fX3lBLBlszxmeVZRzq5Qvg0N8nqvlV3C4CAiv6lEl6_-y6wBoQOWN9OhNhYU6wFjpNNDTx_RW0329i9TYVxaygw58wJGCX_1F5-PY0NG74n_1sdZxYop7s5GnZ0_9S9-DEI-LNR2MMx-oVH4lpGjV5dhGRvZS0l4tMm2C7J6Yx_JoTQoZfWwPI0GGf2smZZ-C2ieB5Wj0b19fgrafuexHW9yeejI51j6WZs_eDqUwvCIf52_yAvokA4SiW4PW8Eod9fX-JuwJQ\nExpire in: 59m59s\n</code></pre> <p>Let's try another variant of the command:</p> <pre><code>kc token --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public --onlyIDToken | kc jwt\n</code></pre> <ul> <li>The <code>--onlyIDToken</code> instruct the command to display only the base64 encoded OIDC token. Useful for batch inclusion.</li> <li>The <code>kc jwt</code> act as a filter to decode the jwt token.</li> </ul> <p>The response should look like:</p> <pre><code>JWT Header:\n{\n  \"alg\": \"RS256\",\n  \"kid\": \"f4ccd454-f3a8-447f-a7c3-67ff935136b1\",\n  \"typ\": \"JWT\"\n}\n\nJWT Payload:\n{\n  \"at_hash\": \"_GWrC20juEb4Zh39S0ly5w\",\n  \"aud\": [\n    \"public\"\n  ],\n  \"auth_time\": 1761564624,\n  \"auth_time_human\": \"2025-10-27 11:30:24 UTC\",\n  \"azp\": \"public\",\n  \"exp\": 1761568224,\n  \"exp_human\": \"2025-10-27 12:30:24 UTC\",\n  \"iat\": 1761564624,\n  \"iat_human\": \"2025-10-27 11:30:24 UTC\",\n  \"iss\": \"https://kubauth.ingress.kubo6.mbp\",\n  \"jti\": \"be30eeb2-153f-4dec-97b8-c75d23035f81\",\n  \"rat\": 1761564624,\n  \"rat_human\": \"2025-10-27 11:30:24 UTC\",\n  \"sub\": \"jim\"\n}\n</code></pre> <p>Notes</p> <p>The <code>auth_time_human</code>, <code>exp_human</code>, <code>iat_human</code> and <code>rat_human</code> are not effective claims,  but values added by the filter to ease interpretation of the corresponding timestamp values.</p> <p>There is also a shortcut (<code>-d</code>) to this command:</p> <pre><code>kc token --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public -d\n</code></pre> <p>NB: Using this option skip the JWT header.</p>"},{"location":"30-user-guide/120-tokens-and-claims/#claims","title":"Claims","text":"<p>A set of 'system' Claims are set by the OIDC server. You will find a description of most of them here.</p> <p>Another important one is the <code>sub</code> token. Stand for 'subject' and is in fact the user's login.</p> <p>Now, type again the previous command, but use john/john123 when prompted for login:</p> <pre><code>JWT Payload:\n{\n  \"at_hash\": \"fdg2po7ht7lBaFFvgXg14A\",\n  \"aud\": [\n    \"public\"\n  ],\n  \"auth_time\": 1761575699,\n  \"auth_time_human\": \"2025-10-27 14:34:59 UTC\",\n  \"azp\": \"public\",\n  \"email\": \"johnd@mycompany.com\",\n  \"emails\": [\n    \"johnd@mycompany.com\"\n  ],\n  \"exp\": 1761579299,\n  \"exp_human\": \"2025-10-27 15:34:59 UTC\",\n  \"iat\": 1761575699,\n  \"iat_human\": \"2025-10-27 14:34:59 UTC\",\n  \"iss\": \"https://kubauth.ingress.kubo6.mbp\",\n  \"jti\": \"822ed082-e615-4153-ad7e-d623df491253\",\n  \"name\": \"John DOE\",\n  \"office\": \"208G\",\n  \"rat\": 1761575699,\n  \"rat_human\": \"2025-10-27 14:34:59 UTC\",\n  \"sub\": \"john\"\n}\n</code></pre> <p>Kubauth has added some new claims, issued from the <code>user</code> resource d\u00e9finition:</p> <ul> <li><code>name</code>: The user <code>spec.name</code> property. Used to to the full user name.</li> <li><code>emails</code>: The user <code>spec.emails</code> list.</li> <li><code>email</code>: The first email of the previous list.</li> <li><code>office</code>: The content of the <code>spec.claims</code> property. Could contains whatever valid map values.</li> </ul> <p>The principle is the resulting set of claim is the result of the merge of:</p> <ul> <li>An initial set of system ones (<code>aud</code>, <code>azp</code>, <code>exp</code>, <code>iss</code>, ...)</li> <li>Some added by Kubauth from the Kubernetes CRD based definition:<ul> <li><code>name</code></li> <li><code>emails</code></li> <li><code>groups</code> (Will be describe in a next chapter)</li> </ul> </li> <li>The content of the user's <code>spec.claims</code></li> </ul> <p>Warning</p> <p>On the current version, claims are not filtered by scope. In other words, all claims of a user are provided in the OIDC token.</p>"},{"location":"30-user-guide/130-audit/","title":"Audit","text":"<p>Kubauth store all login attempts. You can display them with the following <code>kc</code> subcommand:</p> <pre><code>kc audit logins\n</code></pre> <pre><code>WHEN           LOGIN   STATUS            UID   NAME       GROUPS   CLAIMS              EMAILS                  AUTH\nMon 12:22:31   jim     passwordChecked   -                []       {}                  []                      \nMon 12:58:29   john    passwordChecked   -     John DOE   []       {\"office\":\"208G\"}   [johnd@mycompany.com]   \nMon 15:34:59   john    passwordChecked   -     John DOE   []       {\"office\":\"208G\"}   [johnd@mycompany.com]   \n</code></pre> <p>Notes</p> <p>The Claims stored in audit are only the ones bound to the user's Custom Resources.</p> <p>Each attempt is stored as a Kubernetes resource <code>LoginAttempt.kubauth.kubotal.io</code> in the namespace <code>kubauth-login</code></p> <pre><code>kubectl -n kubauth-audit get loginattempts\n</code></pre> <pre><code>NAME                           LOGIN   NAME       STATUS            AUTHORITY   AGE\njim-2025-10-27-11-22-31-017    jim                passwordChecked               3h41m\njohn-2025-10-27-11-58-29-588   john    John DOE   passwordChecked               3h5m\njohn-2025-10-27-14-34-59-923   john    John DOE   passwordChecked               28m\n</code></pre>"},{"location":"30-user-guide/130-audit/#configuration","title":"Configuration","text":"<p>This history is cleaned every 5 minutes by removing all records older than 8 hours. This can be modified by setting a helm chart configuration value:</p> values.yaml <pre><code>oidc:\n  issuer: https://kubauth.ingress.kubo6.mbp\n  postLogoutURL: https://kubauth.ingress.kubo6.mbp/index\n  ...\naudit:\n  namespace: kubauth-audit\n  createNamespace: true\n  cleaner:\n    recordLifetime: \"8h\"\n    cleanupPeriod: \"5m\"\n</code></pre>"},{"location":"30-user-guide/140-sso/","title":"SSO session.","text":"<p>Up to now, we have been prompted with a login dialog each time we issued a <code>kc token ....</code> command.</p> <p>On the Kubauth login page, there is a checkbox <code>Remember me</code>.</p> <p></p> <p>If checked, a session cookie is set, the subsequent login attempt will go through using same user.</p> <p>You can easily check this by launching several <code>kc token ....</code>.</p> <p>The session cookie is related only to the login page. It is independant of the client application. This means you have a cross-application Single Sign On.</p>"},{"location":"30-user-guide/140-sso/#logout","title":"logout","text":"<p>What if you want now to login with another user credential ? </p> <p>Obviously, you must first logout. This can be achieved with another <code>kc</code> subcommand:</p> <p><pre><code>kc logout --issuerURL https://kubauth.ingress.kubo6.mbp \n</code></pre> <pre><code>No OIDC configuration found in kubeconfig\nOpening browser to logout endpoint: https://kubauth.ingress.kubo6.mbp/oauth2/logout\n</code></pre></p> <p>Don't care about the first message, related to kubeconfig. This will be explained later, in Kubernetes integration</p> <p>As prompted, a browser should pop-up to call a specif cleanup URL.</p> <p></p>"},{"location":"30-user-guide/140-sso/#sessions-storage","title":"Sessions storage","text":"<p>Session are persisted as Kubernetes resources:</p> <p><pre><code>kubectl -n kubauth-sso get ssosessions\n</code></pre> <pre><code>NAME                                                                 LOGIN   NAME       DEADLINE               AGE\nh-6d4d8f33c407a727857ba335d257a04d6d0116c40fa81c26903bb9469139563c   john    John DOE   2025-10-28T00:19:44Z   34s\nh-d13bc676b003a200aca5051558b2cd21eccb44f557bee058df00aa3eb6d2bd22   jim                2025-10-28T00:19:13Z   65s\n</code></pre></p> <p>Manually deleting such entry is another method to end the session.</p>"},{"location":"30-user-guide/140-sso/#configuration","title":"Configuration","text":"<p>SSO configuration be modified by setting a helm chart configuration value:</p> values.yaml <pre><code>oidc:\n  issuer: https://kubauth.ingress.kubo6.mbp\n  postLogoutURL: https://kubauth.ingress.kubo6.mbp/index\n  ...\n  sso:\n    namespace: kubauth-sso\n    createNamespace: true\n    lifeTime: \"8h\"\n    sticky: true\n</code></pre> <ul> <li><code>namespace</code> will allow to change the namespace where SSO sessions are persisted. And allow its creation at Kubauth deployment.</li> <li><code>lifetime</code> set the session duration. This is an absolute duration. There is no idle timeout.</li> <li><code>sticky</code> true means session cookie is permanent. If false, the session will be lost on browser close.</li> </ul>"},{"location":"30-user-guide/150-users-groups/","title":"Users Groups","text":"<p>For most application, a Key attribute of a user is the groups this user belong to. To handle this,  Kubauth user management support a Custom Resource: <code>GroupBinding.kubauth.kubotal.io</code></p>"},{"location":"30-user-guide/150-users-groups/#groupbinding","title":"GroupBinding","text":"<p>Here is a sample manifest</p> group1.yaml <pre><code>---\napiVersion: kubauth.kubotal.io/v1alpha1\nkind: GroupBinding\nmetadata:\n  name: jim-dev\n  namespace: kubauth-users\nspec:\n  user: jim\n  group: devs\n</code></pre> <p>This will associate the user <code>jim</code> to the group <code>devs</code>. Note there is no explicit creation of a <code>dev</code> group.  With Kubauth, a group can exist only because it is referenced by a <code>GroupBinding</code>.</p> <p>Notes</p> <p>Generally, Kubernetes does not check referential integrity when creating a resource that references another one. For example, the following will work:</p> <pre><code>kubectl -n ldemo create rolebinding missing-integrity --role=unexisting-role --group=unexisting-group\n\nrolebinding.rbac.authorization.k8s.io/missing-integrity created\n</code></pre> <p>Maybe the referenced resource will be created later, or the link will be useless. This is evidently a design choice in Kubernetes, and Kubauth follows the same logic.</p> <p>Now, apply the manifest:</p> <p><pre><code>kubectl apply -f group1.yaml\n</code></pre> <pre><code>groupbinding.kubauth.kubotal.io/jim-dev created\n</code></pre></p> <p>Now, login with the <code>jim</code> account:</p> <pre><code>kc token --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public -d\n</code></pre> <p>When looking at the decoded JWT token, you should now find a <code>groups</code> claim:</p> <pre><code>.....\nJWT Payload:\n{\n  \"at_hash\": \"bHHRRCaQSInJLqDviiW4Ag\",\n  \"aud\": [\n    \"public\"\n  ],\n  \"auth_time\": 1761643474,\n  \"auth_time_human\": \"2025-10-28 09:24:34 UTC\",\n  \"azp\": \"public\",\n  \"exp\": 1761647074,\n  \"exp_human\": \"2025-10-28 10:24:34 UTC\",\n  \"groups\": [\n    \"devs\"\n  ],\n  \"iat\": 1761643474,\n  \"iat_human\": \"2025-10-28 09:24:34 UTC\",\n  \"iss\": \"https://kubauth.ingress.kubo6.mbp\",\n  \"jti\": \"4a3aed23-e2df-474c-9c26-d8f275d0f51f\",\n  \"rat\": 1761643474,\n  \"rat_human\": \"2025-10-28 09:24:34 UTC\",\n  \"sub\": \"jim\"\n}\n</code></pre> <p>This claim has been added by Kubauth, based on <code>GroupBinding</code> resource.</p> <p>Question</p> <p>What if a <code>groups</code> claim has already being defined in the <code>spec.claims</code> set of the user entity ? </p> <p>Answer: The <code>User.spec.claims.groups</code> is overwritten. </p>"},{"location":"30-user-guide/150-users-groups/#group-custom-resource","title":"Group Custom Resource","text":"<p>Kubauth also allow the explicit creation of <code>Group.kubauth.kubotal.io</code> entity.</p> <p>There will be two reason to do so:</p> <ul> <li>Documentation, by setting a <code>spec.comment</code> attribut on the <code>Group</code> resource.</li> <li>Define a set of 'Claims', which will be applied to all members of the group.</li> </ul> <p>For example, the following manifest will:</p> <ul> <li>Associate user <code>john</code> to the group <code>devs</code> defined previously.</li> <li>Associate user <code>john</code> to the group <code>ops</code>, newly create.</li> <li>Create explicitly the group <code>ops</code></li> <li>with a <code>spec.comment</code> </li> <li>and a <code>spec.claims.accessProfile</code>, which will be set for all members. </li> </ul> group2.yaml <pre><code>---\napiVersion: kubauth.kubotal.io/v1alpha1\nkind: GroupBinding\nmetadata:\n  name: john-dev\n  namespace: kubauth-users\nspec:\n  user: john\n  group: devs\n\n---\napiVersion: kubauth.kubotal.io/v1alpha1\nkind: GroupBinding\nmetadata:\n  name: john-ops\n  namespace: kubauth-users\nspec:\n  user: john\n  group: ops\n\n---\napiVersion: kubauth.kubotal.io/v1alpha1\nkind: Group\nmetadata:\n  name: ops\n  namespace: kubauth-users\nspec:\n  comment:\n  claims:\n    accessProfile: p24x7\n</code></pre> <p>Apply it</p> <p><pre><code>kubectl apply -f group2.yaml\n</code></pre> <pre><code>groupbinding.kubauth.kubotal.io/john-dev created\ngroupbinding.kubauth.kubotal.io/john-ops created\ngroup.kubauth.kubotal.io/ops created\n</code></pre></p> <p>And now, login with the <code>john</code> account:</p> <pre><code>kc token --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public -d\n</code></pre> <pre><code>.....\nJWT Payload:\n{\n  \"accessProfile\": \"p24x7\",\n  \"at_hash\": \"jUGx994P_ESx3KtujtlFLA\",\n  \"aud\": [\n    \"public\"\n  ],\n  \"auth_time\": 1761647376,\n  \"auth_time_human\": \"2025-10-28 10:29:36 UTC\",\n  \"azp\": \"public\",\n  \"email\": \"johnd@mycompany.com\",\n  \"emails\": [\n    \"johnd@mycompany.com\"\n  ],\n  \"exp\": 1761650976,\n  \"exp_human\": \"2025-10-28 11:29:36 UTC\",\n  \"groups\": [\n    \"devs\",\n    \"ops\"\n  ],\n  \"iat\": 1761647376,\n  \"iat_human\": \"2025-10-28 10:29:36 UTC\",\n  \"iss\": \"https://kubauth.ingress.kubo6.mbp\",\n  \"jti\": \"629164c8-75b5-4e16-850f-0975f0afc508\",\n  \"name\": \"John DOE\",\n  \"office\": \"208G\",\n  \"rat\": 1761647376,\n  \"rat_human\": \"2025-10-28 10:29:36 UTC\",\n  \"sub\": \"john\"\n}\n</code></pre> <p>When looking at the decoded JWT token, you should check than:</p> <ul> <li>The <code>groups</code> claim now list two values.</li> <li>The <code>accessProfile</code> claims is set with appropriate value.</li> </ul>"},{"location":"30-user-guide/160-password-grant/","title":"Password Grant (ROPC)","text":"<p>OAuth's Resource Owner Password Credentials (ROPC) grant type, also known as the password grant, was included in the original OAuth 2.0 specification back in 2012 as a temporary migration path. It was designed to help legacy applications transition from HTTP Basic Authentication or direct credential sharing to an OAuth token-based architecture.</p> <p>The OAuth 2.0 Security Best Current Practice (BCP) has since deprecated this flow, and OAuth 2.1 removes it entirely due to several fundamental security concerns:</p> <ul> <li>Credentials exposure: The client application directly handles user passwords</li> <li>Phishing risk: Users are trained to enter passwords into applications</li> <li>Credential theft: If the client is compromised, user passwords are exposed</li> </ul> <p>Despite deprecation, there are legitimate scenarios where ROPC may be the most pragmatic solution:</p> <ul> <li>Constrained CLI/headless sessions: When an interactive browser is impossible (air\u2011gapped shells, jump hosts).</li> <li>Automated scripts and CI/CD pipelines (where service accounts aren't suitable)</li> <li>Terminal-only environments on embedded systems</li> <li>Legacy migrations: Lets you keep an existing UX while you transition server-side to OAuth/OIDC tokens.</li> </ul> <p>For these reasons, ROPC/Password Grant is supported by Kubauth</p>"},{"location":"30-user-guide/160-password-grant/#configuration","title":"Configuration.","text":"<p>While supported, ROPC is unactivated by default. To use it, you must activate it at two levels.</p> <p>First, at global level, by setting a helm chart configuration value:</p> values.yaml <pre><code>oidc:\n  issuer: https://kubauth.ingress.kubo6.mbp\n  postLogoutURL: https://kubauth.ingress.kubo6.mbp/index\n  ....\n  allowPasswordGrant: true\n</code></pre> <p>Then, at the client definition level. Here is a modified version of our public client:</p> client-public.yaml <pre><code>apiVersion: kubauth.kubotal.io/v1alpha1\nkind: OidcClient\nmetadata:\n  name: public\n  namespace: kubauth-oidc\nspec:\n  redirectURIs:\n    - \"http://127.0.0.1:9921/callback\"\n  grantTypes: [ \"refresh_token\", \"authorization_code\", \"password\" ]\n  responseTypes: [ \"id_token\", \"code\", \"token\", \"id_token token\", \"code id_token\", \"code token\", \"code id_token token\" ]\n  scopes: [ \"openid\", \"offline\", \"profile\", \"groups\", \"email\", \"offline_access\" ]\n  description: A test OIDC public client\n  public: true\n\n  # hashedSecret: \"$2a$12$9vdc.xb3Zf4ts/C2pSvIOuGmFiv0EStBJWslaaycavblaIjYZ9Mia\"\n  # accessTokenLifespan: 1h\n  # refreshTokenLifespan: 1h\n  # idTokenLifespan: 1h\n</code></pre> <p>We just added <code>\"password\"</code> to the <code>grantTypes</code> list.</p> <p>Note</p> <p>The <code>redirectURIs</code> list is not used for ROPC flow. Preserving its value will allow to still use <code>authorization_code</code> usual flow.</p> <p>For a client intended to be used only with ROPC flow, the <code>redirectURIs</code> can be empty. (But defined as <code>redirectURIs: []</code> for syntax coherency)</p> <p>Once these two modification successfully applied (<code>helm upgrade ....</code> and <code>kubectl apply ...</code>) , we can use this flow.</p>"},{"location":"30-user-guide/160-password-grant/#kc-token-nui","title":"kc token-nui","text":"<p>Similar to the <code>kc token</code> subcommand, there is a <code>kc token-nui</code> subcommand to generate a token using the ROPC flow:</p> <pre><code>kc token-nui --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public\nLogin:jim\nPassword:\n</code></pre> <p>If authentication is successful, you will get the tokens </p> <pre><code>Access token: ory_at_LAhtO0e8T8-V2wLZ72V0G98jKMJEpYQLH6tm6Aeg_Lw.yZhdVxRSMGnp6FlM63ErD6Lj8vpyJPxxTafNbygAvTE\nRefresh token: ory_rt_kP1rTr6eF_AgdVvUtzfEKywhIddEK2cjDRC9EmkT0Hw.OeeTl_eAgq7jmBbYg6u7MT3P_ukg117nE8RxvZShZRM\nID token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImY0Y2NkNDU0LWYzYTgtNDQ3Zi1hN2MzLTY3ZmY5MzUxMzZiMSIsInR5cCI6IkpXVCJ9.eyJhdWQiOlsicHVibGljIl0sImF1dGhfdGltZSI6MTc2MTY2MDI4NSwiYXpwIjoicHVibGljIiwiZXhwIjoxNzYxNjYzODg1LCJncm91cHMiOlsiZGV2cyJdLCJpYXQiOjE3NjE2NjAyODUsImlzcyI6Imh0dHBzOi8va3ViYXV0aC5pbmdyZXNzLmt1Ym82Lm1icCIsImp0aSI6IjFmMTRiYjkxLTQyYTQtNGUwZC1iNDEzLWE0MDAwN2E4ZmU0ZiIsInJhdCI6MTc2MTY2MDI4NSwic3ViIjoiamltIn0.PaR36uaDqZ0iLm--KZaYjNrD1bE5KI3p5Djo535ig6pC35QakhV4swikN2860koCALiq_Sl5A8Ki0Q7JLNGUp8h67E_ebcs4KVYzbEJq4sK6YfsCRgEEulijiX060DXl76u3hsPq2LEeKo710iIzPTFy_zy9GRTw7vbjk9_NJm3XbVqEw339H-l-yiUO3MKuS7MF6w77tMpw3NHIJ_uDSUC6ZNbXmsGts6N0K2Dy_lEtjij8KG3_zmRDszmV-CzbysUGBy5n0c8LemVRit6JlbwIJs8NvqB-zW_CZhty-3MEJOWitCAdmgcJmLYT_ZRG_YqNnWAWEdwWlZFg_8I6yg\nExpire in: 1h0m0s\n</code></pre> <p>If you got an answer like the following:</p> <pre><code>token request failed with status 403: {\"error\":\"request_forbidden\",\"error_description\":\"The request is not allowed. This server does not  allow to use authorization grant 'password'. Check server configuration\"}\n</code></pre> <p>the configuration steps described above is not effective.</p> <p>For batch inclusion, the <code>kc token-nui</code> subcommand also support <code>--login</code> and/or <code>--password</code> option. And most of the same option as <code>kc token</code>.</p> <p><pre><code>kc token-nui --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public \\\n    --login jim --password jim123 --onlyIDToken\n</code></pre> <pre><code>eyJhbGciOiJSUzI1NiIsImtpZCI6ImY0Y2NkNDU0LWYzYTgtNDQ3Zi1hN2MzLTY3ZmY5MzUxMzZiMSIsInR5cCI6IkpXVCJ9.eyJhdWQiOlsicHVibGljIl0sImF1dGhfdGltZSI6MTc2MTY2MDYxNywiYXpwIjoicHVibGljIiwiZXhwIjoxNzYxNjY0MjE3LCJncm91cHMiOlsiZGV2cyJdLCJpYXQiOjE3NjE2NjA2MTcsImlzcyI6Imh0dHBzOi8va3ViYXV0aC5pbmdyZXNzLmt1Ym82Lm1icCIsImp0aSI6IjY0YjcwNmQ5LWI1MjgtNGRlMy1iM2M2LTVkMjc3MTZjN2FjZSIsInJhdCI6MTc2MTY2MDYxNywic3ViIjoiamltIn0.G2VkVCsWVG1kNFXLp1lWu1ehXzkMJFWDQVANpR1wC8OGpBnwVaoTwRoTjAUw_yUrJu1u_m-NLWyzLIflJfmrTPTN3iz7Jsqc77iFoOmkOBUkOPvp9q66Uu3cbP3e52cYJcgOb5RcvaAOcBdp32zYotSLAPcRSHhuc1K2sdHg96bhU9dR5zs9Z29iXOzez4Bvq2haJpvjz4slZ2FZjSSkswyUlQQdfxoGC_VZgXyAJOycVK-e_oHJSOT1dtCi2y9QEHHHRRX3XpAqvZ86Q3Xk0Loxb03z6VDwzKeH1tYgplAcXTqb9jMmWFdh31JHZGd82S6v9lwUatrEuqo6ZXJR2A\n</code></pre></p>"},{"location":"30-user-guide/170-ldap-connector/","title":"LDAP connector","text":""},{"location":"30-user-guide/170-ldap-connector/#kubauth-architecture","title":"Kubauth architecture","text":"<p>If you look inside the <code>kubauth</code> pod, you will find it is currently made of 3 containers:</p> <p><pre><code>kubectl -n kubauth get pod -l app.kubernetes.io/instance=kubauth\n</code></pre> <pre><code>NAME                       READY   STATUS    RESTARTS   AGE\nkubauth-65dbdbb447-bl4qz   3/3     Running   0          17m\n</code></pre></p> <p>We can list them:</p> <p><pre><code>kubectl -n kubauth get pod -l app.kubernetes.io/instance=kubauth -o jsonpath='{range .items[0].spec.containers[*]}{.name}{\"\\n\"}{end}'\n</code></pre> <pre><code>oidc\naudit\nucrd\n</code></pre></p> <p>Kubauth is build from a modular architecture. The following schema describe the default configuration:</p> <p></p> <p>To exchange together, all modules use the same very simple identity protocol, with only one exchange type.</p> <ul> <li>The Request provide a login and optionally a password</li> <li>The Response provide a status (NotFound, PasswordFail, PasswordChecked, ....) and all user related information (Name, Emails, Claims, ....) it can provide</li> </ul> <p>All communication between modules (aka container) are using <code>localhost</code>. As such they remains 'private' and not exposed outside of the pod.</p> <p>Each module (or container) listen on a specific port</p> <ul> <li>The <code>OIDC</code> module handle all user interaction and initiate an identity request.</li> <li>The <code>audit</code> is a transparent pass trough for the protocol. It log all request to provide the Audit feature described previously</li> <li>The <code>ucrd</code> module will access the User, Groups and GroupBinding Custom resource to build the response.</li> </ul> <p>All these module are 'built-in' the kubauth image. And assembly is performed by the helm chart.</p> <p></p> <p>There is another build-in module: <code>ldap</code> which will serve the identity protocol by connecting to an external LDAP server</p> <p>Here is the new configuration:</p> <p></p> <p>To switch onto this new configuration, we must:</p> <ul> <li>Disable the <code>ucrd</code> module.</li> <li>Activate the <code>ldap</code> module</li> <li>Configure the <code>audit</code> module to request the <code>ldap</code> (Listening on port 6803) module instead of <code>ucrd</code> (Was listening in port 6802)</li> <li>Configure the <code>ldap</code> module depending of the target LDAP Server</li> </ul> <p>The <code>values-ldap.yaml</code> file below setup this configuration.</p> <p>The ldap configuration itself is tailored for an OpenLDAP server deployed as described on Appendix/OpenLDAP deployment</p> <p></p> values-ldap.yaml <pre><code>oidc:\n  issuer: https://kubauth.ingress.kubo6.mbp\n  postLogoutURL: https://kubauth.ingress.kubo6.mbp/index\n  allowPasswordGrant: true\n\n  ingress:\n    host: kubauth.ingress.kubo6.mbp\n\n  server:\n    certificateIssuer: cluster-odp\n\naudit:\n  idProvider:\n    baseURL: http://localhost:6803  # ldap provider listenning port\n\nucrd:\n  enabled: false\n\nldap:\n  enabled: true\n  ldap:\n    host: openldap.openldap.svc\n    insecureNoSSL: true\n    bindDN: cn=admin,dc=mycompany,dc=com\n    bindPW: admin123\n    timeoutSec: 10\n    groupSearch:\n      baseDN: ou=Groups,dc=mycompany,dc=com\n      filter: (objectClass=posixgroup)\n      linkGroupAttr: memberUid\n      linkUserAttr: uid\n      nameAttr: cn\n    userSearch:\n      baseDN: ou=Users,dc=mycompany,dc=com\n      cnAttr: cn\n      emailAttr: mail\n      filter: (objectClass=inetOrgPerson)\n      loginAttr: uid\n      numericalIdAttr: uidNumber\n</code></pre>"},{"location":"30-user-guide/170-ldap-connector/#ldap-configuration","title":"LDAP configuration","text":"<p>If you need to configure another kind of LDAP server, each property is documented in the Helm chart value file. For convenience, below is the corresponding extract:  </p> values-ldap-commented.yaml <pre><code>.......\nldap:\n  enabled: true\n  ldap:\n    # The host and port of the LDAP server.\n    host:\n    #  If port isn't supplied, it will be guessed based on the TLS configuration. (389 or 636).\n    port:\n\n    # Timeout on connection to ldap server. Default to 10\n    timeoutSec:\n    # Required if LDAP host does not use TLS.\n    insecureNoSSL:\n    # Don't verify the CA.\n    insecureSkipVerify:\n    # Connect to the insecure port then issue a StartTLS command to negotiate a\n    # secure connection. If unsupplied secure connections will use the LDAPS protocol.\n    startTLS:\n\n    # Only one of rootCaData, rootCaPath or rootCaSecret must be defined\n    # Path to a trusted root certificate file.\n    rootCaPath:\n    # Base64 encoded PEM data containing root CAs.\n    rootCaData:\n    # A secret hosting the CA, as base64 encoded value, at the 'rootCaSecretPath' location.\n    # The secret can be generated using trust-manager from cert-manager\n    rootCaSecret:\n    rootCaSecretPath:  # Default to 'ca.crt'\n\n    # Path to a client cert file. If LDAP server require a certificate based client authentication\n    clientCert:\n    # Path to a client private key file. If LDAP server require a certificate based client authentication\n    clientKey:\n\n    # BindDN and BindPW for an application service account. The connector uses these\n    # credentials to search for users and groups.\n    bindDN:\n    bindPW:\n\n    groupSearch:\n      # BaseDN to start the search from. For example \"cn=groups,dc=example,dc=com\"\n      baseDN:\n      # Optional filter to apply when searching the directory. For example \"(objectClass=posixGroup)\"\n      filter:\n      # The filter for group/user relationship will be: (&lt;linkGroupAttr&gt;=&lt;Value of LinkUserAttr for the user&gt;)\n      # If there is several value for LinkUserAttr, we will loop on.\n      linkGroupAttr:\n      linkUserAttr:\n      # The attribute of the group that represents its name.\n      nameAttr:\n      # Defaults to \"sub\"\n      scope:\n    userSearch:\n      # BaseDN to start the search from. For example \"cn=users,dc=example,dc=com\"\n      baseDN:\n      #  Optional filter to apply when searching the directory. For example \"(objectClass=person)\"\n      filter:\n      # Attribute to match against the login. This will be translated and combined with the other filter as \"(&lt;loginAttr&gt;=&lt;login&gt;)\".\n      loginAttr:\n      # Can either be:\n      #   \"sub\" - search the whole sub tree\n      #   \"one\" - only search one level\n      scope:\n      # The attribute providing the numerical user ID\n      numericalIdAttr:\n      # The attribute providing the user's email\n      emailAttr:\n      # The attribute providing the user's common name\n      cnAttr:\n</code></pre>"},{"location":"30-user-guide/170-ldap-connector/#deployment","title":"Deployment","text":"<p>One your configuration is ready, you can proceed with its deployment, by launching an <code>helm update ....</code> command:</p> <pre><code>helm -n kubauth upgrade -i kubauth --values ./values-ldap.yaml oci://quay.io/kubauth/charts/kubauth --version 0.1.2-snapshot --create-namespace --wait\n</code></pre> <p>You can check which containers has now been deployed:</p> <p><pre><code>kubectl -n kubauth get pod -l app.kubernetes.io/instance=kubauth -o jsonpath='{range .items[0].spec.containers[*]}{.name}{\"\\n\"}{end}'\n</code></pre> <pre><code>oidc\naudit\nucrd\n</code></pre></p> <p>And you can now test authentication:</p> <p><pre><code>kc token --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public -d\n</code></pre> <pre><code>If browser doesn't open automatically, visit: http://127.0.0.1:9921\n</code></pre></p> <p>If you enter <code>jim/jim123</code>, you will have a login fail.</p> <p>But, if you enter the credentials of a user defined in your LDAP, you should have a successful login. For example, using <code>fred/fred123</code> if using the sample LDAP configuration defined in appendix.</p> <pre><code>Access token: ory_at_2vDnbRl0-cazsy2ZkIV_3bG8blZzgA5Cc0FxKXg5g6g.IvAqwgU04BnmBBoO_IPNt3UtIkiAWfJlgcj5WyMPsPk\nRefresh token: ory_rt_UQQoBvf3Pkk1Yr6W3JCO_aShfX_28e8tAnxcMUQGjJE.I-P_3XJxVwPRmrUDVOXtt0ofS_G2MISSVRMoL48ZIAg\nID token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImY0Y2NkNDU0LWYzYTgtNDQ3Zi1hN2MzLTY3ZmY5MzUxMzZiMSIsInR5cCI6IkpXVCJ9.eyJhdF9oYXNoIjoiM0RMYWZJMzdNZnNGZjVWOUN6QU1hUSIsImF1ZCI6WyJwdWJsaWMiXSwiYXV0aF90aW1lIjoxNzYxNjc0MzU2LCJhenAiOiJwdWJsaWMiLCJlbWFpbCI6ImZyZWRAbXljb21wYW55LmNvbSIsImVtYWlscyI6WyJmcmVkQG15Y29tcGFueS5jb20iXSwiZXhwIjoxNzYxNjc3OTU2LCJncm91cHMiOlsibWFuYWdlcnMiLCJzdGFmZiJdLCJpYXQiOjE3NjE2NzQzNTYsImlzcyI6Imh0dHBzOi8va3ViYXV0aC5pbmdyZXNzLmt1Ym82Lm1icCIsImp0aSI6IjE3YjQxMzk1LTU4MmUtNDM3ZC05YTA2LWNkYWY5NjM1NjdjZiIsIm5hbWUiOiJGcmVkIEFTVEVSIiwicmF0IjoxNzYxNjc0MzU2LCJzdWIiOiJmcmVkIn0.ZSnv4godrQOkrYNaEteOi85N-M9b4UKOI8zj0sorrD5Az1_xezINWiSRkrN2vLq9DtMKXZUBNjr41m4dW83xTsex8Tq_lEtisJP8woZzXzUpv1HnK2_RJiX_euGM2RSUJNaih4vpeDOPBhZQPUTkCIqnH_kus2dLuh84KGmcpLFC7vG60JW1lqRlx1Wb5ggmZmSFHC6k44Lx0bkihhpofiJg0QL1UoxmN1elixOegxOs0g_vxjdoDC3jNVOWONR6qVpvG7YRa25FgNPvfIV1XWeIP9upq0Yn_LNxLXuNFY_ysdw4lIQH2aEO58118KwB5eB8UQRnZJXmnAyKF-OU5Q\nExpire in: 1h0m0s\nJWT Payload:\n{\n  \"at_hash\": \"3DLafI37MfsFf5V9CzAMaQ\",\n  \"aud\": [\n    \"public\"\n  ],\n  \"auth_time\": 1761674356,\n  \"auth_time_human\": \"2025-10-28 17:59:16 UTC\",\n  \"azp\": \"public\",\n  \"email\": \"fred@mycompany.com\",\n  \"emails\": [\n    \"fred@mycompany.com\"\n  ],\n  \"exp\": 1761677956,\n  \"exp_human\": \"2025-10-28 18:59:16 UTC\",\n  \"groups\": [\n    \"managers\",\n    \"staff\"\n  ],\n  \"iat\": 1761674356,\n  \"iat_human\": \"2025-10-28 17:59:16 UTC\",\n  \"iss\": \"https://kubauth.ingress.kubo6.mbp\",\n  \"jti\": \"17b41395-582e-437d-9a06-cdaf963567cf\",\n  \"name\": \"Fred ASTER\",\n  \"rat\": 1761674356,\n  \"rat_human\": \"2025-10-28 17:59:16 UTC\",\n  \"sub\": \"fred\"\n}\n</code></pre> <p>And regarding Audit track record:</p> <p><pre><code>kc audit logins\n</code></pre> <pre><code>WHEN           LOGIN   STATUS            UID   NAME         GROUPS             CLAIMS                                      EMAILS                  AUTH\nTue 18:59:16   fred    passwordChecked   -     Fred ASTER   [staff,managers]   {}                                          [fred@mycompany.com]    \nTue 19:02:20   jim     userNotFound      -                  []                 {}                                          []                    \n</code></pre></p>"},{"location":"30-user-guide/180-several-id-providers/","title":"Several ID Providers","text":"<p>In previous chapter, we substitute our local user database by an external LDAP server. But what if we want to have both.</p>"},{"location":"30-user-guide/180-several-id-providers/#configuration","title":"Configuration","text":"<p>Kubauth provide another built-in module: the <code>merger</code>. </p> <p>Here is our target architecture:</p> <p></p> <p>And here is a <code>values file</code> to implement it:</p> values-merger.yaml <pre><code>oidc:\n  issuer: https://kubauth.ingress.kubo6.mbp\n  postLogoutURL: https://kubauth.ingress.kubo6.mbp/index\n  allowPasswordGrant: true\n\n  ingress:\n    host: kubauth.ingress.kubo6.mbp\n\n  server:\n    certificateIssuer: cluster-odp\n\naudit:\n  idProvider:\n    baseURL: http://localhost:6804 # Merger listening port\n\nmerger:\n  enabled: true\n  idProviders:\n    - name: ldap\n      httpConfig:\n        baseURL: http://localhost:6803 # ldap provider listening port\n    - name: ucrd\n      httpConfig:\n        baseURL: http://localhost:6802 # ucrd provider listening port\n\nucrd:\n  enabled: true\n\nldap:\n  enabled: true\n  ldap:\n    host: openldap.openldap.svc\n    insecureNoSSL: true\n    bindDN: cn=admin,dc=mycompany,dc=com\n    bindPW: admin123\n    timeoutSec: 10\n    groupSearch:\n      baseDN: ou=Groups,dc=mycompany,dc=com\n      filter: (objectClass=posixgroup)\n      linkGroupAttr: memberUid\n      linkUserAttr: uid\n      nameAttr: cn\n    userSearch:\n      baseDN: ou=Users,dc=mycompany,dc=com\n      cnAttr: cn\n      emailAttr: mail\n      filter: (objectClass=inetOrgPerson)\n      loginAttr: uid\n      numericalIdAttr: uidNumber\n</code></pre> <ul> <li>The <code>audit</code> module is now connected to the <code>merger</code> module.</li> <li>This <code>merger</code> module is configured with a list of two ID provider. We will see in next chapter than order is important.</li> <li>The <code>ucrd</code> module is enabled.</li> <li>The <code>ldap</code> module configuration os the same as in previous chapter.</li> </ul> <p>One your configuration is ready, you can proceed with its deployment, by launching an <code>helm update ....</code> command:</p> <pre><code>helm -n kubauth upgrade -i kubauth --values ./values-merger.yaml oci://quay.io/kubauth/charts/kubauth --version 0.1.2-snapshot --create-namespace --wait\n</code></pre> <p>And you can check which containers has now been deployed:</p> <p><pre><code>kubectl -n kubauth get pod -l app.kubernetes.io/instance=kubauth -o jsonpath='{range .items[0].spec.containers[*]}{.name}{\"\\n\"}{end}'\n</code></pre> <pre><code>oidc\naudit\nmerger\nucrd\nldap\n</code></pre></p> <p>If you have followed this manual, the <code>ucrd</code> module has been previously removed. So all its content is gone. You must restore it:</p> <pre><code>kubectl apply -f users.yaml\nkubectl apply -f group1.yaml\nkubectl apply -f group2.yaml \n</code></pre>"},{"location":"30-user-guide/180-several-id-providers/#logins","title":"Logins","text":"<p>And you can now test authentication:</p> <pre><code>kc token --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public -d\n</code></pre> <p>Launch this command twice, one with <code>bob/bob123</code> and another with <code>john/john123</code></p> <p>You can check with the result both user's claims are properly populated.</p>"},{"location":"30-user-guide/180-several-id-providers/#audit","title":"Audit","text":"<p>If you issue some login command:</p> <p><pre><code>kc audit logins\n</code></pre> <pre><code>WHEN           LOGIN   STATUS            UID   NAME         GROUPS       CLAIMS                                      EMAILS                  AUTH\nThu 12:14:32   bob     passwordChecked   -     Bob MORANE   [staff]      {}                                          [bob@mycompany.com]     ldap\nThu 12:14:18   john    passwordChecked   -     John DOE     [devs,ops]   {\"accessProfile\":\"p24x7\",\"office\":\"208G\"}   [johnd@mycompany.com]   ucrd\n</code></pre></p> <p>You can see the rightmost column <code>AUTH</code> for AUTHORITY with is now fulfilled with the name of the ID Provider which validate the password. </p>"},{"location":"30-user-guide/190-merge-of-identity/","title":"Merge of Identity","text":"<p>In previous chapters, each user was defined inside a specific provider database. But Kubauth allow properties of a given user defined in two (or more) ID provider to be merged. </p> <p>This will allow, for example, to enrich a user profile from a central user repository by local (cluster wide) specific properties.</p> <p>This is specially useful if you have a READ ONLY access to your LDAP server.</p>"},{"location":"30-user-guide/190-merge-of-identity/#sample-setup","title":"Sample setup","text":"<p>Here is an example manifest to exercise this capability:</p> <p>It is assumed here than Kubauth is configured with an LDAP and an local CRD user database,    as described in previous chapter, with Users and Groups sample dataset.   And than <code>alice</code> and <code>bob</code> are existing user defined in the LDAP Server.</p> ldap-addon.yaml <pre><code>---\napiVersion: kubauth.kubotal.io/v1alpha1\nkind: User\nmetadata:\n  name: alice\n  namespace: kubauth-users\nspec:\n  passwordHash: \"$2a$12$.WUyue3xr.nKuH8Tu0q.T.WF.PKHLZ92g9ewnLoB.27CuMQIdvuza\" # smith123\n  name: Alice SMITH-WESSON\n  claims:\n    office: 312R\n  emails:\n    - alice@mycompany.com\n    - alice.smith@mycompany.com\n\n---\napiVersion: kubauth.kubotal.io/v1alpha1\nkind: GroupBinding\nmetadata:\n  name: bob-ops\n  namespace: kubauth-users\nspec:\n  user: bob\n  group: ops\n</code></pre> <p>This will:</p> <ul> <li>Create a user <code>alice</code> in the k8S CRD user storage (<code>ucrd</code>). As a duplicate of the existing one in LDAP referential. <code>name</code>, <code>claims</code> and <code>emails</code> properties are provided.</li> <li>Bind the user <code>bob</code> to the group <code>ops</code>. Note than this user does not exist in the <code>ucrd</code> provider.</li> </ul> <p>Apply this manifest:</p> <pre><code>kubectl apply -f ldap-addon.yaml \n</code></pre>"},{"location":"30-user-guide/190-merge-of-identity/#logins","title":"Logins","text":"<p>To ease testing we will use the 'no User Interface' of the <code>kc token</code> tool. Remember, this will use a specific configuration</p>"},{"location":"30-user-guide/190-merge-of-identity/#bob","title":"Bob","text":"<p>First, log as <code>bob</code> user</p> <pre><code>kc token-nui --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public --login bob --password bob123 -d\n</code></pre> <pre><code>JWT Payload:\n{\n  \"accessProfile\": \"p24x7\",\n  \"aud\": [\n    \"public\"\n  ],\n  \"auth_time\": 1761830106,\n  \"auth_time_human\": \"2025-10-30 13:15:06 UTC\",\n  \"authority\": \"ldap\",\n  \"azp\": \"public\",\n  \"email\": \"bob@mycompany.com\",\n  \"emails\": [\n    \"bob@mycompany.com\"\n  ],\n  \"exp\": 1761833706,\n  \"exp_human\": \"2025-10-30 14:15:06 UTC\",\n  \"groups\": [\n    \"ops\",\n    \"staff\"\n  ],\n  \"iat\": 1761830106,\n  \"iat_human\": \"2025-10-30 13:15:06 UTC\",\n  \"iss\": \"https://kubauth.ingress.kubo6.mbp\",\n  \"jti\": \"39600b39-267b-4f31-9179-3f698418e85c\",\n  \"name\": \"Bob MORANE\",\n  \"rat\": 1761830106,\n  \"rat_human\": \"2025-10-30 13:15:06 UTC\",\n  \"sub\": \"bob\"\n}\n</code></pre> <p>One can check here than:</p> <ul> <li>We got a JWT token, so authentication is successful.</li> <li>We have a <code>autority</code> claim. This is added by Kubauth and is the provider name which validate the user's credentials.</li> <li>The <code>groups</code> claim is made of the concatenation of the group list of each provider. With de-duplication</li> <li>There is an <code>accessProfile\": p24x7</code> claim. This is a Claim granted to all members of the group <code>ops</code> defined in the <code>ucrd</code> provider.</li> </ul> <p>As it can be tricky to check from where a value is coming from, Kubauth provide a tool to lookup user details:</p> <p><pre><code>kc audit detail bob\n</code></pre> <pre><code>WHEN           LOGIN   STATUS            UID   NAME         GROUPS        CLAIMS                      EMAILS                AUTH\nThu 14:15:06   bob     passwordChecked   -     Bob MORANE   [ops,staff]   {\"accessProfile\":\"p24x7\"}   [bob@mycompany.com]   ldap\nDetail:\nPROVIDER   STATUS            UID   NAME         GROUPS    CLAIMS                      EMAILS\nldap       passwordChecked   -     Bob MORANE   [staff]   {}                          [bob@mycompany.com]\nucrd       userNotFound      -                  [ops]     {\"accessProfile\":\"p24x7\"}   []\n</code></pre></p> <p>This tool will lookup the last login for the user provided as parameter. It first display a line with the same format as the <code>kc audit logins</code> command. Then, it display another array with, for each provider, which information is provided</p>"},{"location":"30-user-guide/190-merge-of-identity/#alice","title":"alice","text":"<p>Now, login as <code>alice</code> user:</p> <pre><code>kc token-nui --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public --login alice --password alice123 -d\n</code></pre> <pre><code>.....\nJWT Payload:\n{\n  \"aud\": [\n    \"public\"\n  ],\n  \"auth_time\": 1761834557,\n  \"auth_time_human\": \"2025-10-30 14:29:17 UTC\",\n  \"authority\": \"ldap\",\n  \"azp\": \"public\",\n  \"email\": \"alice@mycompany.com\",\n  \"emails\": [\n    \"alice@mycompany.com\",\n    \"alice.smith@mycompany.com\"\n  ],\n  \"exp\": 1761838157,\n  \"exp_human\": \"2025-10-30 15:29:17 UTC\",\n  \"groups\": [\n    \"managers\",\n    \"staff\"\n  ],\n  \"iat\": 1761834557,\n  \"iat_human\": \"2025-10-30 14:29:17 UTC\",\n  \"iss\": \"https://kubauth.ingress.kubo6.mbp\",\n  \"jti\": \"76a841c2-43fa-4635-9702-55c856f75f45\",\n  \"name\": \"Alice SMITH\",\n  \"office\": \"312R\",\n  \"rat\": 1761834557,\n  \"rat_human\": \"2025-10-30 14:29:17 UTC\",\n  \"sub\": \"alice\"\n}\n</code></pre> <p>We can check than:</p> <ul> <li>We are logged using credentials from the LDAP server.</li> <li>The <code>authority</code> claim is set to <code>ldap</code>.</li> <li>The claim <code>office: 312R</code> from the <code>ucrd</code> provider is present</li> <li>The <code>emails</code> claim is made of the concatenation of the emails list of each provider. With de-duplication</li> <li>The <code>name</code> claim is the one from the LDAP definition. While there is another value in the <code>ucrd</code> database,    the value from LDAP take precedence, as the LDAP provider is before in the list.    The value from CRD based definition would have been used if LDAP did not provide any value. </li> </ul> <p>You can also use <code>kc audit detail...</code> to figure out what happen:</p> <p><pre><code>kc audit detail alice\n</code></pre> <pre><code>WHEN           LOGIN   STATUS            UID   NAME          GROUPS             CLAIMS              EMAILS                                            AUTH\nThu 15:29:17   alice   passwordChecked   -     Alice SMITH   [managers,staff]   {\"office\":\"312R\"}   [alice@mycompany.com,alice.smith@mycompany.com]   ldap\nDetail:\nPROVIDER   STATUS            UID   NAME                 GROUPS             CLAIMS              EMAILS\nldap       passwordChecked   -     Alice SMITH          [staff,managers]   {}                  [alice@mycompany.com]\nucrd       passwordFail      -     Alice SMITH-WESSON   []                 {\"office\":\"312R\"}   [alice@mycompany.com,alice.smith@mycompany.com]\n</code></pre></p>"},{"location":"30-user-guide/190-merge-of-identity/#alice-2","title":"alice (2)","text":"<p>Now, let's try to login with the password defined in the <code>ucrd</code> user database: </p> <pre><code>kc token-nui --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public --login alice --password smith123 -d\n</code></pre> <pre><code>token request failed with status 400: {\"error\":\"invalid_grant\",\"error_description\":\"The provided authorization grant \\\n(e.g., authorization code, resource owner credentials) or refresh token is invalid, expired, revoked, \\\ndoes not match the redirection URI used in the authorization request, or was issued to another client. \\\nUnable to authenticate the provided username and password credentials.\"}\n</code></pre> <p><pre><code>kc audit detail alice\n</code></pre> <pre><code>WHEN           LOGIN   STATUS         UID   NAME          GROUPS             CLAIMS              EMAILS                                            AUTH\nThu 16:06:32   alice   passwordFail   -     Alice SMITH   [managers,staff]   {\"office\":\"312R\"}   [alice@mycompany.com,alice.smith@mycompany.com]   ldap\nDetail:\nPROVIDER   STATUS            UID   NAME                 GROUPS             CLAIMS              EMAILS\nldap       passwordFail      -     Alice SMITH          [staff,managers]   {}                  [alice@mycompany.com]\nucrd       passwordChecked   -     Alice SMITH-WESSON   []                 {\"office\":\"312R\"}   [alice@mycompany.com,alice.smith@mycompany.com]\n</code></pre></p> <p>Although this password was validated by the <code>ucrd</code> provider, the login fail.  This is intended behavior, as, for a user defined in a prioritized provider, we don't want a secondary one to allow a password change.</p> <p>In other words, the first provider in the list which define a password for a given user will validate it. (either successfully or not).</p> <p>The providers list order is important</p> <p>This also means the password definition in the k8s CR database for <code>alice</code> is useless.</p>"},{"location":"30-user-guide/190-merge-of-identity/#john","title":"john","text":"<p>Now, let's try to login with <code>john</code> user.</p> <pre><code>kc token-nui --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public --login john --password john123 -d\n</code></pre> <pre><code>.......\nJWT Payload:\n{\n  \"accessProfile\": \"p24x7\",\n  \"aud\": [\n    \"public\"\n  ],\n    .......\n  \"sub\": \"john\"\n}\n</code></pre> <p><pre><code>kc audit detail john\nWHEN           LOGIN   STATUS            UID   NAME       GROUPS       CLAIMS                                      EMAILS                  AUTH\n</code></pre> <pre><code>Thu 16:14:04   john    passwordChecked   -     John DOE   [devs,ops]   {\"accessProfile\":\"p24x7\",\"office\":\"208G\"}   [johnd@mycompany.com]   ucrd\nDetail:\nPROVIDER   STATUS            UID   NAME       GROUPS       CLAIMS                                      EMAILS\nldap       userNotFound      -                []           {}                                          []\nucrd       passwordChecked   -     John DOE   [devs,ops]   {\"accessProfile\":\"p24x7\",\"office\":\"208G\"}   [johnd@mycompany.com]\n</code></pre></p> <p>This time, the login is successful. This because <code>john</code> does not exists on the LDAP side, So the authentication by the <code>ucrd</code> provider is effective.</p> <p>In other words, with the configuration, an administrator which have access to a Kubernetes cluster resource but no access to a central  LDAP server will be able to create local users, but will be unable to change a global user password. </p>"},{"location":"30-user-guide/190-merge-of-identity/#id-provider-configuration","title":"ID Provider configuration","text":""},{"location":"30-user-guide/190-merge-of-identity/#provider-properties","title":"Provider properties","text":"<p>As stated above, provider's order is important. But there is also a way to control which kind of information each provider will add to the user profile.</p> <p>Here is an extract of the values file of the Helm chart, which, by default, configure a single <code>ucrd</code> provider with default values:</p> values.yaml <pre><code>merger:\n  .....\n  idProviders:\n    - name: ucrd\n      httpConfig:\n        baseURL: http://localhost:6802\n      credentialAuthority: true\n      groupAuthority: true\n      groupPattern: \"%s\"\n      claimAuthority: true\n      claimPattern: \"%s\"\n      nameAuthority: true\n      emailAuthority: true\n      critical: true\n      uidOffset: 0\n  ....\n</code></pre> <ul> <li><code>credentialAuthority</code>: Setting this attribute to 'false' will prevent this provider from authenticating any user.</li> <li><code>groupAuthority</code>: Setting this attribute to false will prevent the <code>groups</code> from this provider from being added to each user.</li> <li><code>groupPattern</code>: Allows you to decorate with prefix or postfix all groups provided by this provider. See the example below</li> <li><code>claimAuthority</code>: Setting this attribute to false will prevent the <code>claims</code> from this provider from being added to each user.</li> <li><code>claimPattern</code>: Allows you to decorate with prefix or postfix all claims provided by this provider. This apply only on first level, if the claim is itself a map.</li> <li><code>nameAuthority</code>: Setting this attribute to false will prevent the name from this provider from being added to each user.</li> <li><code>emailAuthority</code>: Setting this attribute to false will prevent <code>emails</code> from this provider from being added to each user.</li> <li><code>critical</code>: Defines the behavior of the chain if this provider is down or out of order (e.g., LDAP server is down). <ul> <li>If true, then all authentication will fail. </li> <li>If false, provider is skipped and authentication is performed as if ot was not existing.</li> </ul> </li> <li><code>uidOffset</code>: This will be added to the UID value if this provider is the authority for this user.</li> </ul> <p>Notes</p> <p>The properties are provided in the helm chart for documentation purpose.  As they match built-in values, the previous snippet is equivalent to </p> <pre><code>merger:\n  .....\n  idProviders:\n    - name: ucrd\n      httpConfig:\n        baseURL: http://localhost:6802\n</code></pre>"},{"location":"30-user-guide/190-merge-of-identity/#example","title":"Example","text":"<p>Given the following requirement:</p> <ul> <li>We want to ensure all users are referenced in a central, corporate repository. (No local users anymore)</li> <li>We still want to be able to enrich users with local attributes (Groups binding, claims, ...)</li> <li>We want to identify all groups which where issued from LDAP.</li> </ul> <p>So:</p> <ul> <li>First point means must prevent local user authentication.</li> <li>Second point means we still need a <code>ucrd</code> K8S CR database to be effective.</li> <li>Third point will be solved by adding a prefix to the groups provided through LDAP.</li> </ul> <p>This can be achieved by modifying the <code>merger</code> configuration in the Helm values file:</p> values-merger.yaml <pre><code>.....\n\nmerger:\n  enabled: true\n  idProviders:\n    - name: ldap\n      groupPattern: \"ldap-%s\"\n      httpConfig:\n        baseURL: http://localhost:6803 # ldap provider listening port\n    - name: ucrd\n      credentialAuthority: false\n      httpConfig:\n        baseURL: http://localhost:6802 # ucrd provider listening port\n.....\n</code></pre> <p>After using an Helm update</p> <pre><code>helm -n kubauth upgrade -i kubauth --values ./values-merger.yaml oci://quay.io/kubauth/charts/kubauth --version 0.1.2-snapshot --create-namespace --wait\n</code></pre> <p>we can now check than:</p> <ul> <li> <p>Users <code>jim</code> and <code>john</code> seems now unexisting.     <pre><code>kc token-nui --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public --login john --password john123 -d\n</code></pre> <pre><code>token request failed with status 400: {\"error\":\"invalid_grant\", ......... Unable to authenticate the provided username and password credentials.\"}\n</code></pre></p> </li> <li> <p>Enrichment of LDAP users attribute are still effective. And the group from LDAP has been prefixed.     <pre><code>kc token-nui --issuerURL https://kubauth.ingress.kubo6.mbp --clientId public --login bob --password bob123 -d\n</code></pre> <pre><code>JWT Payload:\n{\n  \"accessProfile\": \"p24x7\",\n  ....\n  \"authority\": \"ldap\",\n  ......\n  \"groups\": [\n    \"ops\",\n    \"ldap-staff\"\n  ],\n  ....\n}\n</code></pre>   and:     <pre><code>kc audit detail bob\n</code></pre> <pre><code>WHEN           LOGIN   STATUS            UID   NAME         GROUPS             CLAIMS                      EMAILS                AUTH\nSat 10:43:42   bob     passwordChecked   -     Bob MORANE   [ldap-staff,ops]   {\"accessProfile\":\"p24x7\"}   [bob@mycompany.com]   ldap\nDetail:\nPROVIDER   STATUS            UID   NAME         GROUPS         CLAIMS                      EMAILS\nldap       passwordChecked   -     Bob MORANE   [ldap-staff]   {}                          [bob@mycompany.com]\nucrd       N/A               N/A                [ops]          {\"accessProfile\":\"p24x7\"}   []\n</code></pre></p> </li> </ul> <p>Notes</p> <p>As we may use users 'jim' and 'john' later in this manuel, we suggest you bring them alive,  by setting back <code>merger.idProviders[\"ucrd'].credentialAuthority: true</code> and applying again the Helm chart.</p>"},{"location":"40-applications-integration/110-argocd/","title":"ArgoCD Integration","text":""},{"location":"40-applications-integration/110-argocd/#oidc-client-creation","title":"Oidc client creation","text":"<p>As stated in Configuration, a client application is defined as a Kubernetes Custom Resource.</p> <p>So, a manifest like the following should be created:</p> client-argocd.yaml <pre><code>apiVersion: kubauth.kubotal.io/v1alpha1\nkind: OidcClient\nmetadata:\n  name: argocd\n  namespace: kubauth-oidc\nspec:\n  hashedSecret: \"$2a$12$.34NOSBLz9cfW9PD/yjj6uhrvys42Xb4euwKy6UFx9YLYEwxIICAK\" \n  redirectURIs:\n    - \"https://argocd.ingress.kubo6.mbp/auth/callback\"\n    - \"http://localhost:8085/auth/callback\"\n  grantTypes: [ \"refresh_token\", \"authorization_code\" ]\n  responseTypes: [\"id_token\", \"code\", \"token\", \"id_token token\", \"code id_token\", \"code token\", \"code id_token token\"]\n  scopes: [ \"openid\", \"offline\", \"profile\", \"groups\", \"email\", \"offline_access\"]\n  displayName: ArgoCD\n  description: GitOps continuous delivery tool\n  entryURL: https://argocd.ingress.kubo6.mbp/\n</code></pre> <p><code>argocd.ingress.kubo6.mbp</code> must be replaced by your ArgoCD entry point (In 2 locations)</p> <ul> <li>The sample password is 'argocd123'. Thus, the <code>hashedSecret</code> value is the result of a <code>kc hash argocd123</code> command.</li> <li>The <code>http://localhost:8085/auth/callback</code> entry in the <code>redirectURIs</code> list is for the <code>argocd</code> CLI command</li> </ul> <p>Apply this manifest:</p> <pre><code>kubectl apply -f client-argocd.yaml\n</code></pre>"},{"location":"40-applications-integration/110-argocd/#argocd-configuration","title":"ArgoCD configuration","text":"<p>We will assume here ArgoCD is installed using the community provided Helm chart.</p> <p>If ArgoCD is installed using another method, it should be easy to configure OIDC as described in the    ArgoCD manual with information provided here.   </p> <p>This means the configuration is provided using Helm 'values' or 'values files'. As the general ArgoCD configuration is out of the scope of this manual, we will will focus only on the OIDC relative section of the 'values file'.</p> <p>There is two tricky points regarding ArgoCD OIDC configuration:</p> <ul> <li>How to provide the 'secret' value shared by ArgoCD and the Kubauth OIDC server</li> <li>How to provide the CA to validate the Kubauth OIDC issuer URL. </li> </ul>"},{"location":"40-applications-integration/110-argocd/#basic-configuration","title":"Basic configuration","text":"<p>In this case, the secret is provided in clear text, and the CA certificate is provided in place.</p> <p>Create a values file like the following to be added on your Helm command when deploying ArgoCD (<code>.... --values values-kubauth.yaml....</code>).</p> values-kubauth.yaml <pre><code>configs:\n  cm:\n    url: \"https://argocd.ingress.kubo6.mbp\"\n    oidc.config: |\n      name: Kubauth\n      issuer: https://kubauth.ingress.kubo6.mbp\n      clientID: argocd\n      clientSecret: argocd123 \n      requestedScopes: [\"openid\", \"profile\", \"email\", \"groups\"]\n      enablePKCEAuthentication: true\n      # logoutURL: https://kubauth.ingress.kubo6.mbp/oauth2/logout\n      rootCA: |\n        -----BEGIN CERTIFICATE-----\n        MIIGSzCCBDOgAwIBAgIJAN3rPrHNIFfAMA0GCSqGSIb3DQEBCwUAMHUxCzAJBgNV\n        BAYTAkZSMQ4wDAYDVQQIDAVQYXJpczEOMAwGA1UEBwwFUGFyaXMxGTAXBgNVBAoM\n        EE9wZW5EYXRhUGxhdGZvcm0xFjAUBgNVBAsMDUlUIERlcGFydG1lbnQxEzARBgNV\n        BAMMCmNhLm9kcC5jb20wHhcNMjEwODE4MDkyMzA1WhcNMzEwODE2MDkyMzA1WjB1\n        .......\n        .......\n        bCbEcvjOBGCIMC+KrWGLbT3i1e1Lici91aqXcHp9rEZSlO/kPGf5gX6FJcj6jVo7\n        P6KClBmIhVYHMueorH7OUFl8mdsVayxMB8dzlr49yQQzhqif3ywLJQEpClCsbq/d\n        J2D93BTA8z5cto4I5oCtfQ2GjlkfEJG863gcIT/3ieu3AI/+LATFO7+TYVqYY8SI\n        wDQVxs1wOpHZOEekfO4fKW12BQ+f+K9m+j0ISFzUCA==\n        -----END CERTIFICATE-----\n\n  rbac:\n    policy.csv: |\n      g, argocd-admin, role:admin \n</code></pre> <ul> <li><code>argocd.ingress.kubo6.mbp</code> must be replaced by your ArgoCD entry point.</li> <li><code>kubauth.ingress.kubo6.mbp</code> must be replaced by your Kubauth entry point (In 2 locations).</li> </ul> <ul> <li>We enable PKCE, as it is safest and supported by both party.</li> <li>The <code>logoutURL</code> parameter is here commented.. More on this below </li> <li>The <code>rootCA</code> parameter is the CA of the Kubauth issuer. If Kubauth has been installed using Certificate Manager,    as described at the beginning of this manual, it can be retrieved with a command like:   <pre><code>kubectl -n cert-manager get secret cluster-odp-ca -o=jsonpath='{.data.ca\\.crt}' | base64 -d\n</code></pre> <p><code>cluster-odp-ca</code> must be replaced by <code>&lt;your clusterIssuer&gt;-ca</code></p> </li> <li>The <code>rbac</code> subsection grant ArgoCD admin rights to the members of the group <code>argocd-admin</code>.      Of course, access management can be defined in more detail, using ArgoCD RBAC system, but this is out of the scope of this manuel.</li> </ul> <p>Add <code>--values values-kubauth.yaml</code> on your Helm command deploying ArgoCD.</p>"},{"location":"40-applications-integration/110-argocd/#configuration-with-secret","title":"Configuration with secret","text":"<p>In order to better protect the clientSecret, and to avoid in place CA certificate, ArgoCD allow these values to be set in a secret.</p> <p>Modify the 'values file' like the following:</p> values-kubauth.yaml <pre><code>configs:\n  cm:\n    url: \"https://argocd.ingress.kubo6.mbp\"\n    oidc.config: |\n      name: Kubauth\n      issuer: https://kubauth.ingress.kubo6.mbp\n      clientID: argocd\n      clientSecret: $oidc.kubauth.clientSecret\n      requestedScopes: [\"openid\", \"profile\", \"email\", \"groups\"]\n      enablePKCEAuthentication: true\n      # logoutURL: https://kubauth.ingress.kubo6.mbp/oauth2/logout\n      rootCA: $oidc.kubauth.rootCA\n\n  rbac:\n    policy.csv: |\n      g, argocd-admin, role:admin \n</code></pre> <p>And update your Helm chart deployment.</p> <p>The values must now be stored in a secret named <code>argocd-secret</code> (ArgoCD hard coded name) in the argocd namespace.  But in most case, this secret exists and already contains some critical other values. So, care must be taken to just append the new values:</p> <pre><code>kubectl -n argocd create secret generic argocd-secret \\\n  --from-literal='oidc.kubauth.clientSecret=argocd123' \\\n  --from-file=oidc.kubauth.rootCA=./ca.crt \\\n  --dry-run=client -o yaml | kubectl apply -f -\n</code></pre> <p>Don't care about warning message.</p> <p>This assuming <code>./ca.crt</code> contains the CA of the Kubauth issuer</p> <p><pre><code>cat ca.crt\n</code></pre> <pre><code>-----BEGIN CERTIFICATE-----\nMIIGSzCCBDOgAwIBAgIJAN3rPrHNIFfAMA0GCSqGSIb3DQEBCwUAMHUxCzAJBgNV\nBAYTAkZSMQ4wDAYDVQQIDAVQYXJpczEOMAwGA1UEBwwFUGFyaXMxGTAXBgNVBAoM\n.....\nJ2D93BTA8z5cto4I5oCtfQ2GjlkfEJG863gcIT/3ieu3AI/+LATFO7+TYVqYY8SI\nwDQVxs1wOpHZOEekfO4fKW12BQ+f+K9m+j0ISFzUCA==\n-----END CERTIFICATE-----\n</code></pre></p> <p>Refer to the ArgoCD documentation for more information.</p>"},{"location":"40-applications-integration/110-argocd/#usage","title":"Usage","text":"<p>The configuration completed, a 'LOG IN VIA KUBAUTH' button must appears on the ArgoCD entry page.</p> <p>You can log in using whatever defined user. Let's say <code>john</code>. Once logged, you can verify who we are, and the group we belong to, using 'User Info' menu entry.</p> <p>ArgoCD display the email adresse as the Username. </p> <p>Although logged, you can't do anything, without more permission. </p> <p>Now, add the user <code>john</code> to the group <code>argocd-admin</code></p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: kubauth.kubotal.io/v1alpha1\nkind: GroupBinding\nmetadata:\n  name: john-argocd-admin\n  namespace: kubauth-users\nspec:\n  group: argocd-admin\n  user: john\nEOF\n</code></pre> <p>And, after logout and login back, the user <code>john</code> will now be able to act as an ArgoCD administrator. You can create a new project to validate this.</p>"},{"location":"40-applications-integration/110-argocd/#logout-url","title":"Logout URL","text":"<p>With the current configuration, when a user log out from ArgoCD UI, it is redirected to the landing page with the 'LOG IN VIA KUBAUTH' and can log again.</p> <p>Notes</p> <p>If the <code>Remember me</code> checkbox has been checked, you will be log in again automatically by the magic of SSO. If you want to log under another user,  you can cancel your SSO session using a <code>kc logout....</code> command.</p> <p>Now, uncomment the <code>logoutURL: ...</code> entry in the values file \"values-kubauth.yaml\" and update your Helm deployment.</p> <p>Now, on logout, ArgoCD will clean your local context, as previously and redirect your browser on the configured logoutURL. This will have two consequences:</p> <ul> <li>Cleanup your global SSO session</li> <li>Redirect you on a logout page, listing somme of the application (or client) listed on the OidcClients resources (Here only ArgoCD)</li> </ul> <p></p> <p>For an OidcClient application to be presented on this list, three attributes must be set. For ArgoCD:</p> <pre><code>apiVersion: kubauth.kubotal.io/v1alpha1\nkind: OidcClient\n...\nspec:\n  .....\n  displayName: ArgoCD\n  description: GitOps continuous delivery tool\n  entryURL: https://argocd.ingress.kubo6.mbp/\n</code></pre> <p>Notes</p> <p>Kubauth expose also this page on the <code>/index</code> path: <code>https://kubauth.ingress.kubo6.mbp/index</code></p>"},{"location":"40-applications-integration/120-harbor/","title":"Harbor integration","text":""},{"location":"40-applications-integration/120-harbor/#oidc-client-creation","title":"Oidc client creation","text":"<p>As stated in Configuration, a client application is defined as a Kubernetes Custom Resource.</p> <p>So, a manifest like the following should be created:</p> client-harbor.yaml <pre><code>apiVersion: kubauth.kubotal.io/v1alpha1\nkind: OidcClient\nmetadata:\n  name: harbor\n  namespace: kubauth-oidc\nspec:\n  hashedSecret: \"$2a$12$5lLVmTBlougPBAQ8h10wR.W9tvZQBRsK1/Sh48yO8WLyXr.P4w3tm\"\n  redirectURIs:\n    - \"https://harbor.ingress.kubo6.mbp/c/oidc/callback\"\n  grantTypes: [ \"refresh_token\", \"authorization_code\" ]\n  responseTypes: [\"id_token\", \"code\", \"token\", \"id_token token\", \"code id_token\", \"code token\", \"code id_token token\"]\n  scopes: [ \"openid\", \"offline\", \"profile\", \"groups\", \"email\", \"offline_access\"]\n  # Following are optional\n  displayName: Harbor\n  description: Harbor OCI repository\n  entryURL: https://harbor.ingress.kubo6.mbp/\n</code></pre> <p><code>harbor.ingress.kubo6.mbp</code> must be replaced by your Harbor entry point (In 2 locations)</p> <ul> <li>The sample password is 'harbor123'. Thus, the <code>hashedSecret</code> value is the result of a <code>kc hash harbor123</code> command.</li> </ul> <p>Apply this manifest:</p> <pre><code>kubectl apply -f client-harbor.yaml\n</code></pre>"},{"location":"40-applications-integration/120-harbor/#manual-harbor-configuration","title":"Manual Harbor configuration.","text":"<p>We assume here you have a running Harbor installation.</p> <ul> <li>Log in to the Harbor interface with an account that has Harbor system administrator privileges.</li> <li>Under Administration, go to Configuration and select the Authentication tab.</li> <li>Use the Auth Mode drop-down menu to select OIDC.</li> </ul> <p>And set the following values:</p> <ul> <li>Auth Mode: <code>OIDC</code></li> <li>OIDC Provider Name: <code>KUBAUTH</code></li> <li>OIDC Endpoint: <code>https://kubauth.ingrees.kubo6.mbp</code> (To adjust to your local value)</li> <li>OIDC Client ID: <code>harbor</code></li> <li>OIDC Client Secret: <code>harbor123</code></li> <li>Group Claim Name: <code>group</code></li> <li>OIDC Admin Group: <code>harbor-admins</code></li> <li>OIDC Scopes: <code>openid,email,profile,offline_access,groups</code></li> <li>Automatic onboarding: <code>true</code> </li> <li>Username Claim: <code>name</code></li> </ul>"},{"location":"40-applications-integration/120-harbor/#automated-harbor-configuration","title":"Automated Harbor Configuration","text":"<p>OIDC can also be configured using environment variables.</p> values-kubauth.yaml <pre><code>core:\n  extraEnvVars:\n    - name: CONFIG_OVERWRITE_JSON\n      value: |\n        {\n          \"auth_mode\": \"oidc_auth\",\n          \"oidc_name\": \"KUBAUTH\",\n          \"oidc_endpoint\": \"https://kubauth.ingress.kubo6.mbp\",\n          \"oidc_groups_claim\": \"groups\",\n          \"oidc_admin_group\": \"harbor-admins\",\n          \"oidc_client_id\": \"harbor\",\n          \"oidc_client_secret\": \"harbor123\",\n          \"oidc_scope\": \"openid,email,profile,offline_access,groups\",\n          \"oidc_verify_cert\": \"false\",\n          \"oidc_auto_onboard\": \"true\",\n          \"oidc_user_claim\": \"name\"\n        }\n</code></pre>"},{"location":"40-applications-integration/120-harbor/#admin-rights","title":"Admin rights","text":"<p>To grant admin rights to a user, just add it to the groups we have defined as <code>oidc_admin_group</code> in the configuration:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: kubauth.kubotal.io/v1alpha1\nkind: GroupBinding\nmetadata:\n  name: john-harbor-admins\n  namespace: kubauth-users\nspec:\n  group: harbor-admins\n  user: john\nEOF\n</code></pre>"},{"location":"40-applications-integration/130-minio/","title":"MinIO integration","text":""},{"location":"50-kubernetes-integration/110-introduction/","title":"Introduction","text":"<p>This chapter describe how to integrate Kubauth OIDC server to the Kubernetes Authentication system and RBAC.</p> <p>Aim is to authenticate users interacting with the cluster through tools like <code>kubectl</code>.</p> <p>There will be two part for this topic:</p> <ul> <li> <p>We need to configure the Kubernetes API server to interact with Kubauth.</p> <p>A tool to fully automate this configuration is provided. But, manual configuration is also documented.</p> </li> <li> <p>Each user will need to perform a local configuration. </p> <p>Here also, a toolkit is provided to automate this.</p> </li> </ul>"},{"location":"50-kubernetes-integration/120-api-server-configuration/","title":"K8S API Server configuration","text":"<p>https://kubernetes.io/docs/reference/access-authn-authz/authentication/#configuring-the-api-server</p>"},{"location":"50-kubernetes-integration/120-api-server-configuration/#automated-configuration","title":"Automated configuration","text":""},{"location":"50-kubernetes-integration/120-api-server-configuration/#manual-configuration","title":"Manual configuration","text":""},{"location":"70-appendix/100-openldap/","title":"OpenLDAP deployment","text":"<p>If you do not have an easy access to an LDAP server and if you want to exercise Kubauth with this configuration,  one solution could be to deploy an OpenLDAP server on your Kubernetes cluster</p> <p>We may use the following helm chart https://github.com/jp-gouin/helm-openldap for such deployment.</p>"},{"location":"70-appendix/100-openldap/#configuration","title":"Configuration","text":"<p>Here is a sample 'values file': </p> values-openldap.yaml <pre><code>replicaCount: 2\nreplication:\n  enabled: true\n  tls_reqcert: never\nglobal:\n  adminPassword: admin123\n  adminUser: admin\n  configPassword: admin123\n  configUser: admin\n  ldapDomain: mycompany.com\ninitTLSSecret:\n  tls_enabled: false\npersistence:\n  storageClass: standard\nltb-passwd:\n  enabled : false\nphpldapadmin:\n  enabled: true\n  ingress:\n    enabled: true\n    ingressClassName: nginx\n    hosts:\n      - phpldapadmin.ingress.kubo6.mbp\nservice:\n  type: ClusterIP\n</code></pre> <p>If you want to use the phpldapadmin front end, you will have to adjust at least the <code>phpldapadmin.ingress.host</code></p> <p>This deployment is simplified to just fulfill our test requirement. Far to be 'production ready' </p> <ul> <li>There is no encryption. Connection is in clear text.</li> <li>There is no LoadBalancer. LDAP is only accessible from inside the cluster using appropriate service.</li> </ul>"},{"location":"70-appendix/100-openldap/#sample-dataset","title":"Sample dataset","text":"<p>The following other 'values file' will create two groups and three users in a fictitious company.</p> values-ldap-init.yaml <pre><code>customLdifFiles:\n  00-root.ldif: |-\n    # Root creation\n    dn: dc=mycompany,dc=com\n    objectClass: dcObject\n    objectClass: organization\n    o: MyCompany\n  01-base-ou.ldif: |-\n    dn: ou=Users,dc=mycompany,dc=com\n    objectClass: organizationalUnit\n    ou: Users\n\n    dn: ou=Groups,dc=mycompany,dc=com\n    objectClass: organizationalUnit\n    ou: Groups\n  03-first-groups.ldif: |-\n    dn: cn=staff,ou=Groups,dc=mycompany,dc=com\n    objectclass: posixGroup\n    objectclass: top\n    gidnumber: 500\n    cn: staff\n    memberUid: alice\n    memberUid: fred\n    memberUid: bob\n\n    dn: cn=managers,ou=Groups,dc=mycompany,dc=com\n    objectclass: posixGroup\n    objectclass: top\n    gidnumber: 501\n    cn: managers\n    memberUid: alice\n    memberUid: fred\n  05-first-users.ldif: |-\n    dn: uid=alice,ou=Users,dc=mycompany,dc=com\n    cn: Alice SMITH\n    objectclass: inetOrgPerson\n    objectclass: top\n    sn: SMITH\n    uid: alice\n    mail: alice@mycompany.com\n    userpassword: alice123\n\n    dn: uid=bob,ou=Users,dc=mycompany,dc=com\n    cn: Bob MORANE\n    objectclass: inetOrgPerson\n    objectclass: top\n    sn: MORANE\n    uid: bob\n    mail: bob@mycompany.com\n    userpassword: bob123\n\n    dn: uid=fred,ou=Users,dc=mycompany,dc=com\n    cn: Fred ASTER\n    objectclass: inetOrgPerson\n    objectclass: top\n    sn: ASTER\n    uid: fred\n    mail: fred@mycompany.com\n    userpassword: fred123\n</code></pre> <p>As you can see, password are in clear text. </p> <p>OpenLDAP allow to provide them as hashed values</p>"},{"location":"70-appendix/100-openldap/#deployment","title":"Deployment","text":"<p>You can now proceed as usual for an Helm deployment.</p> <p>First, you need to register the helm repo: </p> <pre><code>helm repo add helm-openldap https://jp-gouin.github.io/helm-openldap/\n</code></pre> <p>Then, you can deploy using the two values files.</p> <pre><code>helm -n openldap upgrade -i openldap helm-openldap/openldap-stack-ha \\\n    --values ./values-openldap.yaml --values ./values-ldap-init.yaml \\\n    --version 4.3.3 --create-namespace --wait\n</code></pre> <p>You should now being able to log on the phpldapadmin front end. Use <code>cn=admin,dc=mycompany,dc=com</code> as Login DN and <code>admin123</code> as password.</p>"},{"location":"70-appendix/100-openldap/#update-removal","title":"Update / Removal","text":"<p>If you want to update the configuration, you can issue again an <code>helm update -i .....</code> command. But, if you intend to modify the data set (Users, Groups), this will be uneffective.</p> <p>The Helm chart is build such a way than the dataset is used only on the initial deployment. </p> <p>So, apart from using the frontend, the easiest way to modify it is to uninstall everythings and re-install.</p> <pre><code>helm -n openldap uninstall openldap &amp;&amp;  kubectl delete ns openldap\n</code></pre> <p>The deletion of the namespace is important, as it will cleanup the associated Persistant Volumes. Not doing so will preserve old data.</p>"}]}